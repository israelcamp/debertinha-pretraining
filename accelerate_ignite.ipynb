{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/israel/Documents/debertinha/debenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import get_scheduler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models import initialize_discriminator, initialize_generator, ModelPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainArgs(ModelPaths):\n",
    "    per_device_train_batch_size: int = 1\n",
    "    temperature: float = 1.0\n",
    "    rtd_lambda: float = 20.\n",
    "    tokenizer_name: str = \"debertinha-v2-tokenizer\"\n",
    "    learning_rate: float = 5e-5\n",
    "    mixed_precision: str = \"no\"\n",
    "    weight_decay: float = 0.\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    num_warmup_steps: int = 10_000\n",
    "    lr_scheduler_type: str = \"linear\"\n",
    "    num_train_epochs: int = 1\n",
    "    cpu: bool = False\n",
    "    log_with: str = \"tensorboard\"\n",
    "    project_dir: str = \"debertinha-v2-accelerate\"\n",
    "    max_train_steps: int = None\n",
    "    checkpointing_steps: int = 10\n",
    "    output_dir: str = \"debertinha-v2-checkpoints\"\n",
    "    save_total_limit: int = 1\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "targs = TrainArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(targs.tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataloader(targs, tokenizer, dataset):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm_probability=0.15\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, shuffle=True, collate_fn=data_collator, batch_size=targs.per_device_train_batch_size\n",
    "    )\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('ds_subset_encoded')\n",
    "dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_dataloader(targs, tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['deberta.embeddings.word_embeddings.weight', 'classifier.weight', 'classifier.bias'], unexpected_keys=['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'deberta.embeddings.position_embeddings._weight', 'deberta.embeddings.position_embeddings.weight', 'deberta.embeddings.word_embeddings._weight'])\n",
      "_IncompatibleKeys(missing_keys=['deberta.embeddings.word_embeddings.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'], unexpected_keys=[])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f6a657939d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = initialize_discriminator(targs)\n",
    "generator = initialize_generator(targs)\n",
    "\n",
    "def _set_param(module, param_name, value):\n",
    "    if hasattr(module, param_name):\n",
    "      delattr(module, param_name)\n",
    "    module.register_buffer(param_name, value)\n",
    "\n",
    "def disentangled_hook(module, *inputs):\n",
    "    g_w_ebd = generator.deberta.embeddings.word_embeddings\n",
    "    d_w_ebd = discriminator.deberta.embeddings.word_embeddings\n",
    "    _set_param(d_w_ebd, 'weight', g_w_ebd.weight.detach() + d_w_ebd.weight)\n",
    "\n",
    "discriminator.register_forward_pre_hook(disentangled_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_and_scheduler(model):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": targs.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=targs.learning_rate)\n",
    "\n",
    "    # Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n",
    "    # shorter in multiprocess)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_loader) / targs.gradient_accumulation_steps)\n",
    "    targs.max_train_steps = targs.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=targs.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=targs.num_warmup_steps * targs.gradient_accumulation_steps,\n",
    "        num_training_steps=targs.max_train_steps * targs.gradient_accumulation_steps,\n",
    "    )\n",
    "    return optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer, generator_lr_scheduler = get_optimizer_and_scheduler(generator)\n",
    "discriminator_optimizer, discriminator_lr_scheduler = get_optimizer_and_scheduler(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    mixed_precision=targs.mixed_precision,\n",
    "    gradient_accumulation_steps=targs.gradient_accumulation_steps,\n",
    "    cpu=targs.cpu,\n",
    "    log_with=targs.log_with,\n",
    "    project_dir=targs.project_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator, generator_optimizer, generator_lr_scheduler, discriminator, discriminator_optimizer, discriminator_lr_scheduler, train_loader = accelerator.prepare(\n",
    "    generator, generator_optimizer, generator_lr_scheduler, discriminator, discriminator_optimizer, discriminator_lr_scheduler, train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / targs.gradient_accumulation_steps)\n",
    "targs.num_train_epochs = math.ceil(targs.max_train_steps / num_update_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\n",
    "    \"per_device_train_batch_size\": targs.per_device_train_batch_size,\n",
    "    \"temperature\": targs.temperature,\n",
    "    \"rtd_lambda\": targs.rtd_lambda,\n",
    "    \"tokenizer_name\": targs.tokenizer_name,\n",
    "    \"learning_rate\": targs.learning_rate,\n",
    "    \"mixed_precision\": targs.mixed_precision,\n",
    "    \"weight_decay\": targs.weight_decay,\n",
    "    \"gradient_accumulation_steps\": targs.gradient_accumulation_steps,\n",
    "    \"num_warmup_steps\": targs.num_warmup_steps,\n",
    "    \"lr_scheduler_type\": targs.lr_scheduler_type,\n",
    "    \"num_train_epochs\": targs.num_train_epochs,\n",
    "    \"cpu\": targs.cpu,\n",
    "    \"log_with\": targs.log_with,\n",
    "    \"project_dir\": targs.project_dir,\n",
    "    \"max_train_steps\": targs.max_train_steps,\n",
    "    \"checkpointing_steps\": targs.checkpointing_steps,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.init_trackers(\"mlm_no_trainer\", experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_sampling(logits, topk = 1, temp=1):\n",
    "    top_p = torch.nn.functional.softmax(logits/temp, dim=-1)\n",
    "    topk = max(1, topk)\n",
    "    next_tokens = torch.multinomial(top_p, topk)\n",
    "    return next_tokens, top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(targs.max_train_steps), disable=not accelerator.is_local_main_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:57<00:00,  2.31it/s]"
     ]
    }
   ],
   "source": [
    "completed_steps = 0\n",
    "saved_states = []\n",
    "for epoch in range(0, targs.num_train_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    total_generator_loss = 0\n",
    "    total_discriminator_loss = 0\n",
    "    total_loss = 0\n",
    "    active_dataloader = train_loader\n",
    "    \n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        with accelerator.accumulate(generator, discriminator):\n",
    "            mlm_labels = batch['labels']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            ## GENERATOR STEP\n",
    "            gen_outputs = generator(**batch)\n",
    "            gen_loss = gen_outputs.loss\n",
    "            accelerator.backward(gen_loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(generator.parameters(), targs.max_grad_norm)\n",
    "\n",
    "            generator_optimizer.step()\n",
    "            generator_lr_scheduler.step()\n",
    "            generator_optimizer.zero_grad()\n",
    "\n",
    "            total_generator_loss += gen_loss.detach().float()\n",
    "            ## GENERATOR STEP\n",
    "\n",
    "            ## DISCRIMINATOR BATCH\n",
    "            gen_logits = gen_outputs.logits\n",
    "            gen_logits = gen_logits.view(-1, gen_logits.size(-1))\n",
    "            topk_labels, _ = topk_sampling(gen_logits, topk=1, temp=targs.temperature)\n",
    "            mask_index = (mlm_labels.view(-1)>0).nonzero().view(-1)\n",
    "            top_ids = torch.zeros_like(mlm_labels.view(-1))\n",
    "            top_ids.scatter_(index=mask_index.long(), src=topk_labels.view(-1).long(), dim=-1)\n",
    "            top_ids = top_ids.view(mlm_labels.size())\n",
    "            new_ids = torch.where(mlm_labels>0, top_ids, input_ids).detach()\n",
    "            disc_batch = {\n",
    "                'input_ids': new_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "            }\n",
    "            ## DISCRIMINATOR BATCH\n",
    "\n",
    "            ## DISCRIMINATOR STEP\n",
    "            disc_outputs = discriminator(**disc_batch)\n",
    "            disc_logits = disc_outputs.logits\n",
    "            mask_logits = disc_logits.view(-1)\n",
    "            _input_mask = attention_mask.view(-1).to(mask_logits)\n",
    "            input_idx = (_input_mask>0).nonzero().view(-1)\n",
    "            mask_labels = ((mlm_labels>0) & (mlm_labels!=input_ids)).view(-1)\n",
    "            mask_labels = torch.gather(mask_labels.to(mask_logits), 0, input_idx)\n",
    "            mask_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "            mask_logits = torch.gather(mask_logits, 0, input_idx).float()\n",
    "            disc_loss = targs.rtd_lambda * mask_loss_fn(mask_logits, mask_labels)\n",
    "            accelerator.backward(disc_loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                accelerator.clip_grad_norm_(discriminator.parameters(), targs.max_grad_norm)\n",
    "            discriminator_optimizer.step()\n",
    "            discriminator_lr_scheduler.step()\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            total_discriminator_loss += disc_loss.detach().float()\n",
    "            ## DISCRIMINATOR STEP\n",
    "\n",
    "            total_loss += (gen_loss + disc_loss).detach().float()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if completed_steps % targs.checkpointing_steps == 0:\n",
    "            output_dir = f\"step_{completed_steps}\"\n",
    "            if targs.output_dir is not None:\n",
    "                output_dir = os.path.join(targs.output_dir, output_dir)\n",
    "            accelerator.save_state(output_dir)\n",
    "            saved_states.append(output_dir)\n",
    "\n",
    "            # remove old states directory\n",
    "            if len(saved_states) > targs.save_total_limit:\n",
    "                old_state = saved_states.pop(0)\n",
    "                shutil.rmtree(old_state)\n",
    "\n",
    "        if completed_steps % 100 == 0:\n",
    "            accelerator.log(\n",
    "                {\n",
    "                    \"train_loss\": total_loss.item() / 100,\n",
    "                    \"discriminator_loss\": total_discriminator_loss.item() / 100,\n",
    "                    \"generator_loss\": total_generator_loss.item() / 100,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": completed_steps,\n",
    "                },\n",
    "                step=completed_steps,\n",
    "            )\n",
    "            total_generator_loss = 0\n",
    "            total_discriminator_loss = 0\n",
    "            total_loss = 0\n",
    "\n",
    "        if completed_steps >= targs.max_train_steps:\n",
    "            break\n",
    "\n",
    "    \n",
    "\n",
    "    output_dir = f\"epoch_{epoch}\"\n",
    "    if targs.output_dir is not None:\n",
    "        output_dir = os.path.join(targs.output_dir, output_dir)\n",
    "    accelerator.save_state(output_dir)\n",
    "\n",
    "accelerator.end_training()\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(generator)\n",
    "unwrapped_model.save_pretrained(\n",
    "    \"generator_final\", is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    ")\n",
    "unwrapped_model = accelerator.unwrap_model(discriminator)\n",
    "unwrapped_model.save_pretrained(\n",
    "    \"discriminator_final\", is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
