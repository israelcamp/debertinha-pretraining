{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_procs = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"debertinha-v2-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"ds_subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = dataset.column_names\n",
    "max_seq_length = tokenizer.model_max_length - 2\n",
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tokens(examples):\n",
    "    return {\"tokens\": [tokenizer.tokenize(t) for t in examples[\"text\"]]}\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < max_seq_length  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // max_seq_length) * max_seq_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "def encode_plus(example):\n",
    "    data = defaultdict(list)\n",
    "    for tokens in example[\"tokens\"]:\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        special_tokens_mask = tokenizer.get_special_tokens_mask(input_ids, already_has_special_tokens=True)\n",
    "        data[\"input_ids\"].append(input_ids)\n",
    "        data[\"attention_mask\"].append(attention_mask)\n",
    "        data[\"special_tokens_mask\"].append(special_tokens_mask)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        convert_to_tokens,\n",
    "        batched=True,\n",
    "        num_proc=num_procs,\n",
    "        remove_columns=[\"text\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=num_procs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/20410 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 20410/20410 [00:06<00:00, 3262.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "        encode_plus,\n",
    "        batched=True,\n",
    "        num_proc=num_procs,\n",
    "        remove_columns=[\"tokens\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 20410\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20410/20410 [00:00<00:00, 204555.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"ds_subset_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "debenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
